\documentclass[10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage[ngerman]{babel}
\usepackage[automark]{scrpage2}
\usepackage{amsmath,amssymb,amstext}
%\usepackage{mathtools}
\usepackage[]{color}
\usepackage[]{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage[perpage,para,symbol*]{footmisc}
\usepackage{listings} 
\usepackage[pdfborder={0 0 0},colorlinks=false]{hyperref}
\usepackage[numbers,square]{natbib}
\usepackage{color}
\usepackage{colortbl}
\usepackage[absolute]{textpos}
\usepackage{float}
%\usepackage[colorinlistoftodos,textsize=small,textwidth=2cm,shadow,bordercolor=black,backgroundcolor={red!100!green!33},linecolor=black]{todonotes}

\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt, breaklines=true, showstringspaces=false} 
\restylefloat{figure}

%changehere
\def\titletext{Bericht TT2P 2}
\def\titletextshort{Problem 3}
\author{Steffen Brauer, André Harms,\\ Florian Johannßen, Jan-Christoph Meier,\\ Florian Ocker, Olaf Potratz,\\ Torben Woggan}

\title{\titletext}

%changehere Datum der Übung
\date{10.06.2012}

\pagestyle{scrheadings}
%changehere
\ihead{TT2, Neitzke}
\ifoot{Generiert am:\\ \today}

\cfoot{Steffen Brauer, André Harms,\\ Florian Johannßen, Jan-Christoph Meier,\\ Florian Ocker, Olaf Potratz,\\ Torben Woggan}


\ohead[]{\titletextshort}
\ofoot[]{{\thepage} / \pageref{LastPage}}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\begin{document}
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents

%	\listoftables                                 												% 
	\listoffigures  
%	\lstlistoflistings	
\newpage
\section{Frage 1}

\section{Frage 2}
Warum nähert sicher das vom On-Policy-Verfahren SARSA erzeugte Verhalten dem des Off-Policy-Verfahrens Q-Learning bei kleinem Epsilon an?

Die Güte eines Zustands wird bei beiden Verfahren durch die Aktions-Wertfunktion Q bestimmt. Nach jedem Aktions-Schritt wird diese entsprechend der gemachten Erfahrungen angepasst. Bei der Berechnung von Q fließt bei Sarsa anteilig (in abgeschwächter Form) die Erfahrung der Folgeaktion mit ein. Es wird also ein Schritt in die Zukunft geschaut und dort eine Aktion gemäß Strategie gewählt. Der Gewinn eben dieser Folgeaktion hat also Einfluss auf die Bewertung der vorangegangenen Aktion, welche in diesen Zustand führen wird. 

Während der Lernphase der Verfahren verfolgen diese in der Regel eine Epsilon-Greedy-Strategie. Diese Strategie wählt immer die, bezüglich des Gewinns, beste Aktion aus, hat jedoch eine Wahrscheinlichkeit Epsilon mit der sie eine zufällig andere Aktion auswählt. Diesen Experimentiervorgang nennt man Explorieren, da nun alternative Möglichkeiten ausprobiert werden. Bei einem hohen Epsilon-Wert exploriert ein Agent entsprechend häufiger, typisch ist jedoch ein konstanter, kleiner Wert (z.B. Epsilon = 0,1).
Da SARSA beim Bewerten von Q auch die Folgeaktion einbezieht (dies nennt man On-Policy), kann durch eine explorative Folgeaktionen, die einen schlechten Gewinn zur Folge hat, die ursprüngliche Aktion schlecht bewertet werden. Diese Aktion muss jedoch für die Zielereichung nicht ungünstig sein (und ist es häufig auch nicht), sie bietet möglichweise nur die Option auf eine schlechte Aktion. Folgendes Beispiel veranschaulicht dieses Phänomen. Angenommen ein Agent muss eine Hängebrücke überqueren, um ein Ziel zu erreichen. Jedoch besteht die Möglichkeit bei einer wenig sinnvollen Aktion von der Brücke zu fallen, was eine extrem negative Bewertung zur Folge hätte. Durch den Explorationsmechanismus würde der Agent dies von Zeit zur Zeit tun. Dadurch würde aber die Aktion, welche zum Betreten der Brücke führt, ebenfalls schlecht bewertet werden. Dieser Algorithmus führt letztendlich dazu, dass der Agent einen möglichst "sicheren" Weg wählt bei dem keine negativen Aktionen zu erwarten sind.

Q-Learning benutzt das Off-Policy-Verfahren, was bedeutet, dass für die Auswahl und für die Bewertung einer Aktion unterschiedliche Strategien zum Einsatz kommen. Die Wahl der Aktion erfolgt auch hier Epsilon-Greedy. Bei der anteiligen Einbeziehung der Erfahrungen durch die Folgeaktion wird jedoch immer die beste Aktion gewählt, selbst wenn dort möglicherweise anschließend exploriert werden sollte. Die tatsächliche Folgeaktion hat also keinen Einfluss auf die Bewertung der ersten Aktion, sondern lediglich die Güte des Folgezustands.

Wird nun ein sehr kleines Epsilon gewählt, so wird auch seltener Exploriert. Dadurch ist die Folgeaktion auch häufiger die beste wählbare Aktion (gemäß ihres Erwartungswerts). Der Unterschied in den Bewertungsverfahren wird dementsprechend geringer und die beiden Verfahren nähern sich in ihrem Verhalten an.

\section{Frage 3}
Wie geht man vor, wenn die Zustandsvariablen, die einen Zustand beschreiben kontinuierlich sind? Wie geht man vor, wenn der Zustandsraum so groß ist, dass der Speicher nicht reicht, um alle V-Werte vzw. Q-Werte abzuspeichern.

Ist die tabellarische Verwaltung der Zustandswerte nicht möglich, da es zuviele Zustände gibt oder sie kontinuierlich sind, müssen sie generalisiert werden. Zusätzlich muss die V- oder Q-Funktion approximiert werden.




\section{Frage 4}
Da bei Kniffel nicht die letzten Zustände sondern alle Aktionen entscheidend für eine Bewertung des Spiels sind, benötigt man ein Verfahren, das alle Status-Kombinationen bewertet. Hier bietet sich Monte-Carlo an. Q-Learning würde nur die unmittelbare Belohnung einer Status-Aktions-Kombination berücksichtigen.
Monte-Carlo hingegen speichert alle Status-Kombinationen, die durchlaufen
werden. Sobald ein Status eine Belohnung gibt, wird diese Belohnung auf alle gesammelten Status-Actions-Kombinationen übertragen. Die Übertragung findet hierbei nicht eins zu eins statt, sondern bildet das Mittel über alle Q-Werte, die bereits vorher ermittelt wurden.

\end{document}

